{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7DibmRRay9F1"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamaramrt/Mamo.ia/blob/main/adapta%C3%A7%C3%A3o_codigo_wylk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalação das Bibliotecas:**"
      ],
      "metadata": {
        "id": "7DibmRRay9F1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Instalação das bibliotecas utilizadas no código:*\n",
        "\n",
        "*   *[fpdf](https://pyfpdf.readthedocs.io/en/latest): Geração de PDFs com Python.*\n",
        "*   *[gradio](https://www.gradio.app/docs): Interface gráfica para melhor interação.*\n",
        "*   *[matplotlib](https://matplotlib.org): Visualização de dados.*\n",
        "*   *[numpy](https://numpy.org/doc): Tratamento numérico de de arrays.*\n",
        "*   *[opencv-python](https://docs.opencv.org/4.x): Tratamento de imagens e visão computacional.*\n",
        "*   *[pandas](https://pandas.pydata.org/docs): Tratamento com estruturas de dados, DataFrames e tabelas.*\n",
        "*   *[pillow](https://pillow.readthedocs.io/en/stable): Tratamento de imagens.*\n",
        "*   *[pyTorch](https://docs.pytorch.org/docs/stable/index.html): Ferramentas de redes neurais.*\n",
        "*   *[pywavelets](https://pywavelets.readthedocs.io/en/latest/#): Tratamento de imagens e sinais, com a transformada wavelet.*\n",
        "*   *[seaborn](https://seaborn.pydata.org): Visualização de dados.*\n",
        "*   *[scikit-learn](https://scikit-learn.org/stable): Ferramentas de Machine Learning, regressão linear, clustering, etc.*\n",
        "*   *[tensorflow](https://www.tensorflow.org/api_docs): Ferramentas de Inteligência Artificial e Machine Learning.*\n",
        "\n",
        "*Outras bibliotecas que já estão inclusas no Google Colab e serão utilizadas:*\n",
        "\n",
        "*   *[google.colab](https://colab.research.google.com/notebooks/snippets/accessing_files.ipynb): Tratamento de arquivos, como upload ou Google Drive.*\n",
        "*   *[io](https://docs.python.org/3/library/io.html): Tratamento de fluxo de dados de entrada e saída.*\n",
        "*   *[os](https://docs.python.org/3/library/os.html): Tratamento com o sistema operacional.*\n",
        "*   *[zipfile](https://docs.python.org/3/library/zipfile.html): Tratamento de arquivos do formato ZIP.*"
      ],
      "metadata": {
        "id": "ikucQtOvzTjb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt72sj9My5Jc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install fpdf gradio matplotlib numpy opencv-python pandas pillow pywavelets scikit-learn seaborn tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importação das Bibliotecas:**"
      ],
      "metadata": {
        "id": "_Q0PGq2s8p21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import io\n",
        "import os\n",
        "import pywt\n",
        "import zipfile\n",
        "\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "# import tensoflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "from fpdf import FPDF\n",
        "from google.colab import files\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets"
      ],
      "metadata": {
        "id": "bddRTC1r8tMG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "8b6734c8-9b2a-4372-ead3-bc977f369834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fpdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3721053166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfpdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fpdf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funções Auxiliares**"
      ],
      "metadata": {
        "id": "jHQJP2Z299VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para extrair o arquivo ZIP\n",
        "def extrair_zip(caminho_zip : str, destino : str):\n",
        "    \"\"\"\n",
        "    Parametros:\n",
        "        input:\n",
        "        caminho_zip [str]: Caminho do arquivo ZIP.\n",
        "        destino [str]: Caminho para a pasta de destino.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(file=caminho_zip, mode='r') as zip_ref:\n",
        "        zip_ref.extractall(destino)\n",
        "\n",
        "# Função para carregar e pré-processar as imagens\n",
        "def carregar_imagens(caminho_pasta : str):\n",
        "    \"\"\"\n",
        "    Parametros:\n",
        "        input:\n",
        "        caminho_pasta [str]: Caminho da pasta\n",
        "\n",
        "        output:\n",
        "        np.array(imagens) [np.ndarray]: Array com as imagens da pasta.\n",
        "        np.array(rotulos) [np.ndarray]: Array com os rótulos das imagens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Lista para as imagens e rótulos\n",
        "    imagens = []\n",
        "    rotulos = []\n",
        "\n",
        "    # Percorre a pasta extraída\n",
        "    for pasta in os.listdir(caminho_pasta):\n",
        "\n",
        "        # Percorre as subpastas\n",
        "        caminho_pasta_imagens = os.path.join(caminho_pasta, pasta)\n",
        "        if os.path.isdir(caminho_pasta_imagens):\n",
        "            for imagem_nome in os.listdir(caminho_pasta_imagens):\n",
        "                # Caminho da imagem\n",
        "                caminho_imagem = os.path.join(caminho_pasta_imagens, imagem_nome)\n",
        "\n",
        "                # Leitura da imagem com opencv-python\n",
        "                imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                # Verifica se a variavel é vazia\n",
        "                if imagem is None:\n",
        "                    continue\n",
        "                else:\n",
        "                    # Redimensiona a imagem para 224x224\n",
        "                    imagem = cv2.resize(imagem, (224, 224))\n",
        "\n",
        "                    # Adiciona a imagem e rótulo a lista\n",
        "                    imagens.append(imagem)\n",
        "                    rotulos.append(pasta)\n",
        "\n",
        "    # Retorno da função\n",
        "    return np.array(imagens), np.array(rotulos)\n",
        "\n",
        "# Função para aplicar a DWT\n",
        "def aplicar_dwt(imagem : np.ndarray):\n",
        "    \"\"\"\n",
        "    Parametros:\n",
        "        input:\n",
        "        imagem [np.ndarray]: Array do tipo NumPy representando a imagem\n",
        "\n",
        "        output:\n",
        "        np.concatenate([LL.flatten(), LH.flatten(), HL.flatten(), HH.flatten()]) [np.ndarray]: Coeficientes da DWT.\n",
        "    \"\"\"\n",
        "\n",
        "    # Realizando a transformada DWT\n",
        "    coeffs2 = pywt.dwt2(imagem, 'bior1.3')\n",
        "    LL, (LH, HL, HH) = coeffs2\n",
        "    return np.concatenate([LL.flatten(), LH.flatten(), HL.flatten(), HH.flatten()])"
      ],
      "metadata": {
        "id": "ZbTwRM1Z-EXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rede Neural - LeNet:**"
      ],
      "metadata": {
        "id": "kSZIjSty1q_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da arquitetura da LeNet ajustada para CIFAR-100\n",
        "class LeNetCIFAR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetCIFAR, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 20, kernel_size=5)  # Primeira camada convolucional para imagens RGB\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Camada de pooling\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5) # Segunda camada convolucional\n",
        "        self.fc1 = nn.Linear(50 * 5 * 5, 500)  # Primeira camada totalmente conectada\n",
        "        self.fc2 = nn.Linear(500, 100)  # Camada de saída para 100 classes (cifar-100)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 50 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QulARf0Tcjpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações da LeNet\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(50 * 4 * 4, 500) #penultima camada (onde vai ser feita a extracao de atributos)\n",
        "        self.fc2 = nn.Linear(500, 2) # nesse caso so temos duas classes (T1 e T2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 50 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7rKZX-Q2yaMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tratamento dos Arquivos:**"
      ],
      "metadata": {
        "id": "wiUuGz2nvUsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminhos utilizados\n",
        "caminho_zip = 'mama.zip'\n",
        "destino = 'mama_extraida'\n",
        "\n",
        "# Processo de extração e preparação dos dados\n",
        "extrair_zip(caminho_zip, destino)\n",
        "imagens, rotulos = carregar_imagens(destino)"
      ],
      "metadata": {
        "id": "Odv3UtRivajp",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "473bc2de-8274-4475-f210-358ca5cdc6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extrair_zip' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2588432012.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Processo de extração e preparação dos dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mextrair_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaminho_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestino\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimagens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotulos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcarregar_imagens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestino\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extrair_zip' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-Treinamento:**"
      ],
      "metadata": {
        "id": "huLXyfG_b64u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transformações para a normalização das imagens e conversao para tensores\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Carregar dataset CIFAR-100\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Criar dataloaders (carregar os dados em lotes)\n",
        "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Inicializar o modelo, função de perda e otimizador\n",
        "model = LeNetCIFAR().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Função para treinar o modelo\n",
        "def train_model(num_epochs : int = 10):\n",
        "\n",
        "    # Executando o modelo para cada época\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Calculando as perdas\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}\")\n",
        "\n",
        "    # Salvando o modelo\n",
        "    torch.save(model.state_dict(), 'lenet_cifar100.pth')\n",
        "    print(\"Modelo salvo como 'lenet_cifar100.pth'.\")\n",
        "\n",
        "train_model(num_epochs=10)"
      ],
      "metadata": {
        "id": "uoJLNeHwb-PZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Montagem do Drive:**"
      ],
      "metadata": {
        "id": "7FZXlzyjfd5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb3nY3jCfYCX",
        "outputId": "4bea3f60-f361-4bad-b533-427ae441b622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extração de Atributos:**"
      ],
      "metadata": {
        "id": "E5WTLndveuc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processamento da imagem para torná-la compatível com a Lenet\n",
        "def preprocessamento_imagem(image_path : str):\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize((32, 32)),                            # Redimensiona imagem para 32x32\n",
        "                                    transforms.ToTensor(),                                  # Converte a imagem em tensor\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalização\n",
        "                                    ])\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Função para extrair atributos\n",
        "def extrair_atributos_img(image_path : str, model):\n",
        "\n",
        "    input_image = preprocessamento_imagem(image_path).to(device)\n",
        "\n",
        "    # Modo avaliação do modelo\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Passar pela conv1, conv2 e obter a penúltima camada\n",
        "        x = model.pool(torch.relu(model.conv1(input_image)))\n",
        "        x = model.pool(torch.relu(model.conv2(x)))\n",
        "        x = x.view(-1, 50 * 5 * 5)\n",
        "\n",
        "        # Extrair os atributos da penúltima camada\n",
        "        features = torch.relu(model.fc1(x))\n",
        "\n",
        "    # Converter para lista e retornar\n",
        "    return features.squeeze().cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "x658U_vhexfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instanciar a LeNet e carregar os pesos pré-treinados\n",
        "model = LeNet().to(device)\n",
        "model.load_state_dict(torch.load('lenet_cifar100.pth'))  # Carrega o modelo treinado\n",
        "\n",
        "# Caminho para a imagem de exemplo\n",
        "image_path = '/content/Lesao_Benigna_T2_71.jpg'\n",
        "\n",
        "# Extração de atributos\n",
        "features = extrair_atributos_img(image_path, model)\n",
        "print(f\"Atributos extraídos: {features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "aUBzx9wbfMJg",
        "outputId": "0924a94a-f406-4954-a9b1-fe257042681d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3562948290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Instanciar a LeNet e carregar os pesos pré-treinados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lenet_cifar100.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Carrega o modelo treinado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo:**"
      ],
      "metadata": {
        "id": "r6tX2cH3vyGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "me3jjdmW0OO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gráfico de Métricas:**"
      ],
      "metadata": {
        "id": "_ae7AyWv2_86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação\n",
        "perda, acuracia = modelo.evaluate(X_test, y_test)\n",
        "print(f'\\n✅ Acurácia no conjunto de teste: {acuracia * 100:.2f}%\\n')\n",
        "\n",
        "# Relatório de classificação\n",
        "y_pred = np.argmax(modelo.predict(X_test), axis=1)\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "# Matriz confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
        "disp.plot(cmap='Blues', xticks_rotation=45)\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Métricas\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, labels=np.unique(y_test))\n",
        "classes = le.classes_\n",
        "\n",
        "# Plot das métricas\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(classes))\n",
        "width = 0.25\n",
        "\n",
        "\n",
        "plt.bar(x - width, precision, width=width, label='Precisão', color='skyblue')\n",
        "plt.bar(x, recall, width=width, label='Recall', color='orange')\n",
        "plt.bar(x + width, f1, width=width, label='F1-score', color='green')\n",
        "\n",
        "plt.xticks(x, classes, rotation=45)\n",
        "plt.ylabel('Pontuação')\n",
        "plt.title('Métricas por Classe')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KakzHvyU3B8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interface Gráfica:**"
      ],
      "metadata": {
        "id": "-xDjWd9x2OMu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-4BiHrx2N47"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}